{
	"metadata": {
		"accelerator": "GPU",
		"colab": {
			"name": "Applio NoUI",
			"private_outputs": true,
			"gpuType": "T4",
			"collapsed_sections": [
				"section_install",
				"section_inference",
				"section_training",
				"section_inference_export"
			]
		},
		"kernelspec": {
			"display_name": "Python 3",
			"name": "python3"
		},
		"language_info": {
			"name": "python"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 0,
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "0pKllbPyK_BC"
			},
			"source": [
				"# <font class=\"markdown-google-sans\" size=6>**Applio NoUI**</font>\n",
				"> <font class=\"markdown-google-sans\">A simple, high-quality voice conversion tool focused on ease of use and performance.</font>\n",
				"\n",
				"<!--a href=\"https://discord.gg/urxFjYmYYh\"><img src=\"https://img.shields.io/discord/1096877223765606521?style=for-the-badge&logo=discord&logoColor=7289DA&labelColor=white&color=7289DA&label=Support\"></a-->\n",
				"<a href=\"https://discord.gg/urxFjYmYYh\"><img src=\"https://img.shields.io/badge/Support-gray?logo=Discord&style=for-the-badge&labelColor=black&logoColor=white\"></a>&emsp;<a href=\"https://github.com/IAHispano/Applio\"><img src=\"https://img.shields.io/badge/GitHub-gray?logo=GitHub&style=for-the-badge&labelColor=black&logoColor=white\"></a>&emsp;<a href=\"https://github.com/IAHispano/Applio/blob/main/TERMS_OF_USE.md\"><img src=\"https://img.shields.io/badge/Terms-gray?logo=ReadTheDocs&style=for-the-badge&labelColor=black&logoColor=white\"></a>\n",
				"\n",
				"---\n",
				"\n",
				"## <font class=\"markdown-google-sans\">ü§ù **Acknowledgments**</font>\n",
				"\n",
				"To all external collaborators for their special help in the following areas:\n",
				"* Hina (Encryption method)\n",
				"* Poopmaster (Extra section)\n",
				"* Shirou (UV installer)\n",
				"* Bruno5430 (AutoBackup code, general notebook maintenance)\n",
				"* kubinka0505 (notebook revamp)\n",
				"\n",
				"---\n",
				"\n",
				"## <font class=\"markdown-google-sans\">‚ö†Ô∏è **Disclaimer**</font>\n",
				"> By using Applio, you agree to comply with ethical and legal standards, respect intellectual property and privacy rights, avoid harmful or prohibited uses, and accept full responsibility for any outcomes, while Applio disclaims liability and reserves the right to amend these terms."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "section_install"
			},
			"source": [
				"# <font class=\"markdown-google-sans\">**Install Applio**</font>\n",
				"If the runtime restarts, re-run the installation steps."
			]
		},
		{
			"cell_type": "code",
			"source": [
				"#@title ## üå± <font class=\"markdown-google-sans\">Initialize variables</font>\n",
				"import os\n",
				"import torch\n",
				"import warnings\n",
				"\n",
				"Git_Repository = \"IAHispano/Applio\" #@param {type: \"string\"}\n",
				"Git_Repository = Git_Repository.replace(os.sep, \"/\")\n",
				"Use_Legacy_Backup_Directory_Names = True #@param {type: \"boolean\"}\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"Title = Git_Repository.split(\"/\")[-1]\n",
				"\n",
				"Path_Base = os.getcwd()\n",
				"Path_Code = os.path.join(Path_Base, Title)\n",
				"Path_Drive_Parent = os.path.join(Path_Base, \"drive\")\n",
				"Path_Drive_Full = os.path.join(Path_Drive_Parent, \"MyDrive\")\n",
				"\n",
				"Path_Name_Backup = (\"\" if Use_Legacy_Backup_Directory_Names else os.sep).join((Title, \"Backup\"))\n",
				"Path_Name_RVC_Backup = \"RVC_Backup\"\n",
				"\n",
				"Path_Full_Backup = os.path.join(Path_Drive_Full, Path_Name_Backup)\n",
				"Path_Logs = os.path.join(Path_Base, Title, \"logs\")\n",
				"\n",
				"GPU = torch.cuda.is_available()\n",
				"GPU_Warning = 'Runtime type is not set to \"GPU\", many actions will not be possible'\n",
				"GPU_Error = \"GPU is unavailable\"\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"def gather_patterns(directory: str, patterns: list):\n",
				"\tfiles = []\n",
				"\n",
				"\tfor pattern in list(patterns):\n",
				"\t\tfor file in Path(directory).glob(pattern.strip(\".\")):\n",
				"\t\t\tfiles.append(str(file.resolve()))\n",
				"\n",
				"\treturn files\n",
				"\n",
				"def find_latest_file_in_directory(directory: str, patterns: list):\n",
				"\tfiles = gather_patterns(directory, patterns)\n",
				"\n",
				"\tif files:\n",
				"\t\treturn sorted(files, key = os.path.getmtime)[-1]\n",
				"\telse:\n",
				"\t\treturn None"
			],
			"metadata": {
				"id": "u8xk1XLUtUWK",
				"cellView": "form"
			}
		},
		{
			"cell_type": "code",
			"metadata": {
				"id": "CAXW55BQm0PP",
				"cellView": "form"
			},
			"source": [
				"#@title ## ‚öôÔ∏è <font class=\"markdown-google-sans\">Setup runtime environment</font>\n",
				"import os\n",
				"import shutil\n",
				"import warnings\n",
				"from multiprocessing import cpu_count\n",
				"\n",
				"if not GPU:\n",
				"\twarnings.warn(GPU_Warning)\n",
				"\n",
				"cwd = os.getcwd()\n",
				"# os.chdir(Path_Base)\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"Reinstall = True #@param {type: \"boolean\"}\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"CPU_Cores = cpu_count()\n",
				"Post_Process = False\n",
				"\n",
				"os.environ[\"CPU_Cores\"] = str(CPU_Cores)\n",
				"\n",
				"#LOGS_PATH = \"/content/Applio/logs\"\n",
				"#BACKUPS_PATH = \"/content/drive/MyDrive/ApplioBackup\"\n",
				"\n",
				"#is os.path.exists(Path_Code):\n",
				"#\tshutil.rmtree(Path_Code)\n",
				"\n",
				"!git config --global advice.detachedHead false\n",
				"\n",
				"if Reinstall:\n",
				"\tshutil.rmtree(Path_Code, ignore_errors = True)\n",
				"\n",
				"!git clone https://github.com/{Git_Repository} --branch 3.6.0 --single-branch\n",
				"os.chdir(Path_Code)\n",
				"print()\n",
				"\n",
				"# Install older Python\n",
				"if os.sys.version_info.minor > 11:\n",
				"\tprint(\"Installing older Python...\")\n",
				"\t!apt update -y\n",
				"\t!apt install -y python3.11 python3.11-distutils python3.11-dev portaudio19-dev 7zip\n",
				"\t!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 2\n",
				"\t!update-alternatives --set python3 /usr/bin/python3.11\n",
				"\tos.sys.path.append(\"/usr/local/lib/python3.11/dist-packages\")\n",
				"\tprint()\n",
				"\n",
				"print(\"Installing requirements...\")\n",
				"!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
				"!uv pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu128 --index-strategy unsafe-best-match\n",
				"!uv pip install jupyter-ui-poll py7zr\n",
				"print()\n",
				"\n",
				"!python \"core.py\" \"prerequisites\" \\\n",
				"\t--models \"True\" \\\n",
				"\t--pretraineds_hifigan \"True\"\n",
				"print()\n",
				"\n",
				"print(\"Finished installing requirements!\")"
			]
		},
		{
			"cell_type": "code",
			"metadata": {
				"id": "yFhAeKGOp9aa",
				"cellView": "form"
			},
			"source": [
				"#@title ## üíΩ <font class=\"markdown-google-sans\">Mount Drive</font>\n",
				"import os\n",
				"import shutil\n",
				"from google.colab import drive\n",
				"\n",
				"try:\n",
				"\tdrive.mount(Path_Drive_Parent, force_remount = True)\n",
				"except Exception as e:\n",
				"\tprint(f\"Failed to mount drive due to {e.__class__.__name__}: {e}\")\n",
				"\n",
				"os.chdir(Path_Base)\n",
				"cwd = os.getcwd()\n",
				"\n",
				"# Migrate directories to match documentation\n",
				"if os.path.ismount(Path_Drive_Parent):\n",
				"\tos.chdir(Path_Drive_Full)\n",
				"\n",
				"\tif all((\n",
				"\t\tnot os.path.exists(Path_Name_Backup),\n",
				"\t\tos.path.exists(Path_Name_RVC_Backup)\n",
				"\t)):\n",
				"\t\tshutil.move(Path_Name_RVC_Backup, Path_Name_Backup)\n",
				"\n",
				"os.chdir(cwd)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "section_inference"
			},
			"source": [
				"# ‚ö° <font class=\"markdown-google-sans\">**Inference**</font>\n"
			]
		},
		{
			"cell_type": "code",
			"metadata": {
				"cellView": "form",
				"id": "section_inference_syncdrive"
			},
			"source": [
				"#@title ### ü§ù Sync with Google Drive\n",
				"#@markdown > Run this cell to automatically Save / Load models from your mounted drive.\n",
				"#@markdown >\n",
				"#@markdown > This will merge and link your backup folder from Google Drive to this notebook\n",
				"import os\n",
				"import shutil\n",
				"import tempfile\n",
				"import warnings\n",
				"import subprocess\n",
				"from pathlib import Path\n",
				"from IPython.display import display, clear_output\n",
				"\n",
				"if not GPU:\n",
				"\twarnings.warn(GPU_Warning)\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"non_bak_folders = \"mute\", \"reference\", \"zips\", \"mute_spin\", \"mute_spin-v2\"\n",
				"non_bak_path = os.path.join(tempfile.gettempdir(), \"Applio\", \"rvc\", \"logs\")\n",
				"\n",
				"def press_button(button):\n",
				"\tbutton.disabled = True\n",
				"\n",
				"def get_date(path):\n",
				"\tfrom datetime import datetime\n",
				"\treturn datetime.fromtimestamp(int(os.stat(path).st_mtime))\n",
				"\n",
				"def get_size(path):\n",
				"\t# use du for consistency with original behavior\n",
				"\tresult = subprocess.run(\n",
				"\t\t[\"du\", \"-shx\", \"--apparent-size\", path],\n",
				"\t\tcapture_output = True,\n",
				"\t\ttext = True\n",
				"\t)\n",
				"\treturn result.stdout.split(\"\\t\")[0] + \"B\"\n",
				"\n",
				"def sync_folders(folder, backup):\n",
				"\tfrom time import sleep\n",
				"\tfrom ipywidgets import widgets\n",
				"\tfrom jupyter_ui_poll import ui_events\n",
				"\n",
				"\tlocal = widgets.VBox([\n",
				"\t\twidgets.Label(f\"Local: {Path_Logs.removeprefix(os.getcwd())}/{os.path.basename(folder)}/\"),\n",
				"\t\twidgets.Label(f\"Size: {get_size(folder)}\"),\n",
				"\t\twidgets.Label(f\"Last modified: {get_date(folder)}\")\n",
				"\t])\n",
				"\n",
				"\tremote = widgets.VBox([\n",
				"\t\twidgets.Label(f\"Remote: {Path_Name_RVC_Backup.removeprefix(os.getcwd())}/{os.path.basename(backup)}/\"),\n",
				"\t\twidgets.Label(f\"Size: {get_size(backup)}\"),\n",
				"\t\twidgets.Label(f\"Last modified: {get_date(backup)}\")\n",
				"\t])\n",
				"\n",
				"\tseparator = widgets.VBox([\n",
				"\t\twidgets.Label(\"|||\"),\n",
				"\t\twidgets.Label(\"|||\"),\n",
				"\t\twidgets.Label(\"|||\")\n",
				"\t])\n",
				"\n",
				"\tradio = widgets.RadioButtons(\n",
				"\t\toptions = [\n",
				"\t\t\t\"Save local model to drive\",\n",
				"\t\t\t\"Keep remote model\"\n",
				"\t\t]\n",
				"\t)\n",
				"\n",
				"\tbutton = widgets.Button(\n",
				"\t\tdescription = \"Sync\",\n",
				"\t\ticon = \"upload\",\n",
				"\t\ttooltip = \"Sync model\"\n",
				"\t)\n",
				"\n",
				"\tbutton.on_click(press_button)\n",
				"\n",
				"\tclear_output()\n",
				"\tprint(f\"Your local model '{os.path.basename(folder)}' is in conflict with it's copy in Google Drive.\")\n",
				"\tprint(\"Please select which one you want to keep:\")\n",
				"\tdisplay(widgets.Box([local, separator, remote]))\n",
				"\tdisplay(radio)\n",
				"\tdisplay(button)\n",
				"\n",
				"\twith ui_events() as poll:\n",
				"\t\twhile not button.disabled:\n",
				"\t\t\tpoll(10)\n",
				"\t\t\tsleep(0.1)\n",
				"\n",
				"\tif radio.value.lower().startswith(\"save\"):\n",
				"\t\tif os.path.exists(backup):\n",
				"\t\t\tshutil.rmtree(backup)\n",
				"\t\tshutil.move(folder, backup)\n",
				"\telif radio.value.lower().startswith(\"keep\"):\n",
				"\t\tif os.path.exists(folder):\n",
				"\t\t\tshutil.rmtree(folder)\n",
				"\n",
				"if os.path.ismount(Path_Drive_Parent):\n",
				"\tos.makedirs(Path_Name_RVC_Backup, exist_ok = True)\n",
				"\tos.makedirs(non_bak_path, exist_ok = True)\n",
				"\n",
				"\tif not os.path.islink(Path_Logs):\n",
				"\t\tfor folder_name in non_bak_folders:\n",
				"\t\t\tfolder = os.path.join(Path_Logs, folder_name)\n",
				"\t\t\tbackup = os.path.join(Path_Name_RVC_Backup, folder_name)\n",
				"\n",
				"\t\t\tos.makedirs(folder, exist_ok=True)\n",
				"\n",
				"\t\t\ttry:\n",
				"\t\t\t\tshutil.move(folder, non_bak_path)\n",
				"\t\t\texcept Exception:\n",
				"\t\t\t\tpass\n",
				"\n",
				"\t\t\tif os.path.exists(folder):\n",
				"\t\t\t\tshutil.rmtree(folder, ignore_errors=True)\n",
				"\n",
				"\t\t\tfolder = os.path.join(non_bak_path, folder_name)\n",
				"\n",
				"\t\t\tif os.path.exists(backup):\n",
				"\t\t\t\tif os.path.realpath(backup) != os.path.realpath(folder):\n",
				"\t\t\t\t\tshutil.rmtree(backup)\n",
				"\n",
				"\t\t\ttry:\n",
				"\t\t\t\tos.symlink(folder, backup)\n",
				"\t\t\texcept FileExistsError:\n",
				"\t\t\t\tpass\n",
				"\n",
				"\t\t# pathlib used only for directory iteration\n",
				"\t\tfor model in Path(Path_Logs).iterdir():\n",
				"\t\t\tif model.is_dir() and not model.is_symlink():\n",
				"\t\t\t\tif model.name == \".ipynb_checkpoints\":\n",
				"\t\t\t\t\tcontinue\n",
				"\n",
				"\t\t\t\tbackup = os.path.join(Path_Name_RVC_Backup, model.name)\n",
				"\n",
				"\t\t\t\tif os.path.isdir(backup):\n",
				"\t\t\t\t\tsync_folders(str(model), backup)\n",
				"\t\t\t\telse:\n",
				"\t\t\t\t\tif os.path.exists(backup):\n",
				"\t\t\t\t\t\tos.remove(backup)\n",
				"\n",
				"\t\t\t\t\tshutil.move(str(model), backup)\n",
				"\n",
				"\t\tshutil.rmtree(Path_Logs)\n",
				"\t\tos.symlink(Path_Name_RVC_Backup, Path_Logs)\n",
				"\n",
				"\t\tclear_output()\n",
				"\t\tprint(\"‚úÖ Models are synced!\")\n",
				"\n",
				"\telse:\n",
				"\t\tif os.path.exists(Path_Logs):\n",
				"\t\t\tshutil.rmtree(Path_Logs)\n",
				"\n",
				"\t\tif os.path.lexists(Path_Logs):\n",
				"\t\t\tif os.path.islink(Path_Logs) or os.path.isfile(Path_Logs):\n",
				"\t\t\t\tos.remove(Path_Logs)\n",
				"\t\t\telse:\n",
				"\t\t\t\tshutil.rmtree(Path_Logs)\n",
				"\n",
				"\t\tos.symlink(Path_Name_RVC_Backup, Path_Logs)\n",
				"\n",
				"\t\tclear_output()\n",
				"\t\tprint(\"‚úÖ Models already synced!\")\n",
				"else:\n",
				"\tprint(\"‚ùå Drive is not mounted, skipping model syncing\")\n",
				"\tprint(\"To sync your models, first mount your Google Drive and re-run this cell\")"
			]
		},
		{
			"cell_type": "code",
			"metadata": {
				"cellView": "form",
				"id": "v0EgikgjFCjE"
			},
			"source": [
				"#@title ### üì• <font size=\"markdown-google-sans\">Download Model</font>\n",
				"#@markdown > Hugging Face or Google Drive\n",
				"import os\n",
				"import warnings\n",
				"\n",
				"if not GPU:\n",
				"\twarnings.warn(GPU_Warning)\n",
				"\n",
				"cwd = os.getcwd()\n",
				"os.chdir(Path_Code)\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"#@markdown > Separateble by `,`.\n",
				"Model_Link = \"\"  # @param {type:\"string\"}\n",
				"Models_Links = Model_Link.split(\",\")\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"for Link in Models_Links:\n",
				"\t!python \"core.py\" \"download\" \\\n",
				"\t\t--model_link \"{Link}\"\n",
				"\n",
				"\tif Link != Models_Links[-1]:\n",
				"\t\tprint()\n",
				"\n",
				"os.chdir(cwd)"
			]
		},
		{
			"cell_type": "code",
			"source": [
				"#@title ### ‚ö° <font class=\"markdown-google-sans\">Run Inference</font>\n",
				"#@markdown <font color=red>**Requires GPU.**</font>\n",
				"import os\n",
				"import torch\n",
				"import warnings\n",
				"import soundfile as sf\n",
				"from IPython.display import Audio, HTML, display\n",
				"\n",
				"if not GPU:\n",
				"\traise Exception(GPU_Error)\n",
				"\n",
				"cwd = os.getcwd()\n",
				"os.chdir(Path_Code)\n",
				"\n",
				"#-=-=-=-#\n",
				"#@markdown > Attempts to find the `.pth` file in the directory below.\n",
				"#@markdown >\n",
				"#@markdown > Model `.index` is also found automatically.\n",
				"Model_Directory = \"/content/Applio/logs/Darwin\" #@param {type: \"string\"}\n",
				"if os.path.isfile(Model_Directory):\n",
				"\twarnings.warn(\"Model directory is a file, re-set to its dirname\")\n",
				"\tprint()\n",
				"\tModel_Directory = os.path.dirname(Model_Directory)\n",
				"\n",
				"Path_Model_Inference = find_latest_file_in_directory(Model_Directory, [\"*.pth\"])\n",
				"if not Path_Model_Inference:\n",
				"\traise Exception(\"Model not found\")\n",
				"\n",
				"Path_Index_Inference = find_latest_file_in_directory(Model_Directory, [\"*.index\"])\n",
				"if not Path_Index_Inference:\n",
				"\twarnings.warn(\"Index not found\")\n",
				"\n",
				"try:\n",
				"\ttorch.load(Path_Model_Inference, map_location = \"cpu\", weights_only = True)\n",
				"except Exception as e:\n",
				"\traise Exception(\"Invalid model\")\n",
				"\n",
				"#-=-=-=-#\n",
				"#@markdown ---\n",
				"#@markdown ### üí¨ Inputs\n",
				"#@markdown > Both can be files.\n",
				"#@markdown >\n",
				"#@markdown > <font color=gold>‚ö†Ô∏è **If `Path_Input` is directory then `Path_Output` has to be directory as well.**</font>\n",
				"\n",
				"Path_Input = \"/content/drive/MyDrive/RVC/Data/Inputs/1\" #@param {type: \"string\"}\n",
				"Path_Output = \"/content/drive/MyDrive/RVC/Data/Outputs/1\" #@param {type: \"string\"}\n",
				"\n",
				"Path_Model_Inference = find_latest_file_in_directory(Model_Directory, [\"*.pth\"])\n",
				"os.environ[\"INFER_Path_Model_Inference\"] = str(Path_Model_Inference)\n",
				"\n",
				"Path_Index_Inference = find_latest_file_in_directory(Model_Directory, [\"*.index\"])\n",
				"os.environ[\"INFER_Path_Index_Inference\"] = str(Path_Index_Inference)\n",
				"\n",
				"#-=-=-=-#\n",
				"#@markdown ---\n",
				"#@markdown ### üîä Playback\n",
				"Autoplay = \"No\" #@param [\"Input\", \"Output\", \"Output after Input\", \"No\"]\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"# test loading model\n",
				"try:\n",
				"\ttorch.load(Path_Model_Inference, map_location = \"cpu\", weights_only = True)\n",
				"except Exception:\n",
				"\traise Exception(\"Invalid model\")\n",
				"\n",
				"# prepare input/output\n",
				"Files_Final = []\n",
				"\n",
				"if os.path.isfile(Path_Input):\n",
				"\tFiles_Final.append(Path_Input)\n",
				"\tPath_Output_Final = Path_Output\n",
				"else:\n",
				"\tif os.path.isfile(Path_Output):\n",
				"\t\traise Exception(\"Input is a directory but output is a file\")\n",
				"\n",
				"\tos.makedirs(Path_Output, exist_ok=True)\n",
				"\n",
				"\tfor File in Path(Path_Input).glob(\"*.*\"):\n",
				"\t\tFile = str(File.resolve())\n",
				"\t\ttry:\n",
				"\t\t\tsf.SoundFile(File)\n",
				"\t\texcept Exception:\n",
				"\t\t\tprint(f\"File {File} is not a valid audio file, skipping\")\n",
				"\t\t\tcontinue\n",
				"\t\tFiles_Final.append(File)\n",
				"\n",
				"# remove previous outputs if any\n",
				"shutil.rmtree(Path_Output, ignore_errors = True)\n",
				"os.makedirs(Path_Output, exist_ok = True)\n",
				"\n",
				"print(f\"{'Using model:':<15} {Path_Model_Inference}\")\n",
				"print(f\"{'Using index:':<15} {Path_Index_Inference}\")\n",
				"print()\n",
				"\n",
				"# Run inference\n",
				"for File in Files_Final:\n",
				"\tPath_Input_Final = File\n",
				"\n",
				"\t# Determine actual output file path\n",
				"\tif os.path.isdir(Path_Output):\n",
				"\t\tbase_name = os.path.splitext(os.path.basename(File))[0]\n",
				"\t\tPath_Output_Final = os.path.join(Path_Output, f\"{base_name}.{Export_Format.lower()}\")\n",
				"\telse:\n",
				"\t\tPath_Output_Final = Path_Output\n",
				"\n",
				"\tos.environ[\"INFER_Path_Input_Final\"] = Path_Input_Final\n",
				"\tos.environ[\"INFER_Path_Output_Final\"] = Path_Output_Final\n",
				"\n",
				"\t#print(\"Processing:\", Path_Input_Final)\n",
				"\n",
				"\tif Post_Process:\n",
				"\t\t!python \"core.py\" \"infer\" \\\n",
				"\t\t\t--pitch \"$INFER_Pitch\" \\\n",
				"\t\t\t--volume_envelope \"$INFER_Volume_Envelope\" \\\n",
				"\t\t\t--index_rate \"$INFER_Search_Feature_Ratio\" \\\n",
				"\t\t\t--protect \"$INFER_Protect_Voiceless_Consonants\" \\\n",
				"\t\t\t--f0_autotune \"$INFER_Autotune\" \\\n",
				"\t\t\t--f0_autotune_strength \"$INFER_Autotune_Strength\" \\\n",
				"\t\t\t--f0_method \"$INFER_Pitch_Extraction_Algorithm\" \\\n",
				"\t\t\t--input_path \"$INFER_Path_Input_Final\" \\\n",
				"\t\t\t--output_path \"$INFER_Path_Output_Final\" \\\n",
				"\t\t\t--pth_path \"$INFER_Path_Model_Inference\" \\\n",
				"\t\t\t--index_path \"$INFER_Path_Index_Inference\" \\\n",
				"\t\t\t--split_audio \"$INFER_Split_Audio\" \\\n",
				"\t\t\t--clean_audio \"$INFER_Clean_Audio\" \\\n",
				"\t\t\t--clean_strength \"$INFER_Clean_Strength\" \\\n",
				"\t\t\t--export_format \"$INFER_Export_Format\" \\\n",
				"\t\t\t--embedder_model \"$INFER_Embedder_Model\" \\\n",
				"\t\t\t--embedder_model_custom \"$INFER_Embedder_Model_Custom\" \\\n",
				"\t\t\t--formant_shift \"$INFER_Formant_Shifting\" \\\n",
				"\t\t\t--formant_qfrency \"$INFER_Formant_Quefrency\" \\\n",
				"\t\t\t--formant_timbre \"$INFER_Formant_Timbre\" \\\n",
				"\t\t\t--post_process \"$INFER_Post_Process\" \\\n",
				"\t\t\t--reverb \"$INFER_Reverb\" \\\n",
				"\t\t\t--pitch_shift \"$INFER_Pitch_Shift\" \\\n",
				"\t\t\t--limiter \"$INFER_Limiter\" \\\n",
				"\t\t\t--gain \"$INFER_Gain\" \\\n",
				"\t\t\t--distortion \"$INFER_Distortion\" \\\n",
				"\t\t\t--chorus \"$INFER_Chorus\" \\\n",
				"\t\t\t--bitcrush \"$INFER_BitCrush\" \\\n",
				"\t\t\t--clipping \"$INFER_Clipping\" \\\n",
				"\t\t\t--compressor \"$INFER_Compressor\" \\\n",
				"\t\t\t--delay \"$INFER_Delay\" \\\n",
				"\t\t\t--reverb_room_size \"$INFER_Reverb_Room_Size\" \\\n",
				"\t\t\t--reverb_damping \"$INFER_Reverb_Damping\" \\\n",
				"\t\t\t--reverb_wet_gain \"$INFER_Reverb_Wet_Gain\" \\\n",
				"\t\t\t--reverb_dry_gain \"$INFER_Reverb_Dry_Gain\" \\\n",
				"\t\t\t--reverb_width \"$INFER_Reverb_Width\" \\\n",
				"\t\t\t--reverb_freeze_mode \"$INFER_Reverb_Freeze_Mode\" \\\n",
				"\t\t\t--pitch_shift_semitones \"$INFER_Pitch_Shift_Semitones\" \\\n",
				"\t\t\t--limiter_threshold \"$INFER_Limiter_Threshold\" \\\n",
				"\t\t\t--limiter_release_time \"$INFER_Limiter_Release_Time\" \\\n",
				"\t\t\t--gain_db \"$INFER_Gain_dB\" \\\n",
				"\t\t\t--distortion_gain \"$INFER_Distortion_Gain\" \\\n",
				"\t\t\t--chorus_rate \"$INFER_Chorus_Rate\" \\\n",
				"\t\t\t--chorus_depth \"$INFER_Chorus_Depth\" \\\n",
				"\t\t\t--chorus_center_delay \"$INFER_Chorus_Center_Delay\" \\\n",
				"\t\t\t--chorus_feedback \"$INFER_Chorus_Feedback\" \\\n",
				"\t\t\t--chorus_mix \"$INFER_Chorus_Mix\" \\\n",
				"\t\t\t--bitcrush_bit_depth \"$INFER_BitCrush_Bit_Depth\" \\\n",
				"\t\t\t--clipping_threshold \"$INFER_Clipping_Threshold\" \\\n",
				"\t\t\t--compressor_threshold \"$INFER_Compressor_Threshold\" \\\n",
				"\t\t\t--compressor_ratio \"$INFER_Compressor_Ratio\" \\\n",
				"\t\t\t--compressor_attack \"$INFER_Compressor_Attack\" \\\n",
				"\t\t\t--compressor_release \"$INFER_Compressor_Release\" \\\n",
				"\t\t\t--delay_seconds \"$INFER_Delay_Seconds\" \\\n",
				"\t\t\t--delay_feedback \"$INFER_Delay_Feedback\" \\\n",
				"\t\t\t--delay_mix \"$INFER_Delay_Mix\"\n",
				"\telse:\n",
				"\t\t!python \"core.py\" \"infer\" \\\n",
				"\t\t\t--pitch \"$INFER_Pitch\" \\\n",
				"\t\t\t--volume_envelope \"$INFER_Volume_Envelope\" \\\n",
				"\t\t\t--index_rate \"$INFER_Search_Feature_Ratio\" \\\n",
				"\t\t\t--protect \"$INFER_Protect_Voiceless_Consonants\" \\\n",
				"\t\t\t--f0_autotune \"$INFER_Autotune\" \\\n",
				"\t\t\t--f0_autotune_strength \"$INFER_Autotune_Strength\" \\\n",
				"\t\t\t--f0_method \"$INFER_Pitch_Extraction_Algorithm\" \\\n",
				"\t\t\t--input_path \"$INFER_Path_Input_Final\" \\\n",
				"\t\t\t--output_path \"$INFER_Path_Output_Final\" \\\n",
				"\t\t\t--pth_path \"$INFER_Path_Model_Inference\" \\\n",
				"\t\t\t--index_path \"$INFER_Path_Index_Inference\" \\\n",
				"\t\t\t--split_audio \"$INFER_Split_Audio\" \\\n",
				"\t\t\t--clean_audio \"$INFER_Clean_Audio\" \\\n",
				"\t\t\t--clean_strength \"$INFER_Clean_Strength\" \\\n",
				"\t\t\t--export_format \"$INFER_Export_Format\" \\\n",
				"\t\t\t--embedder_model \"$INFER_Embedder_Model\" \\\n",
				"\t\t\t--embedder_model_custom \"$INFER_Embedder_Model_Custom\" \\\n",
				"\t\t\t--formant_shifting \"$INFER_Formant_Shift\" \\\n",
				"\t\t\t--formant_qfrency \"$INFER_Formant_Quefrency\" \\\n",
				"\t\t\t--formant_timbre \"$INFER_Formant_Timbre\" \\\n",
				"\t\t\t--post_process \"$INFER_Post_Process\"\n",
				"\n",
				"\tif os.path.exists(Path_Output_Final):\n",
				"\t\t# read audio length in seconds\n",
				"\t\tinput_duration = sf.info(Path_Input_Final).duration\n",
				"\t\toutput_duration = sf.info(Path_Output_Final).duration\n",
				"\t\tgap = 0.25  # seconds between playbacks\n",
				"\n",
				"\t\tautoplay_lower = Autoplay.lower()\n",
				"\n",
				"\t\tif autoplay_lower == \"input\":\n",
				"\t\t\tdisplay(Audio(Path_Input_Final, autoplay = True))\n",
				"\t\t\tdisplay(Audio(Path_Output_Final, autoplay = False))\n",
				"\t\telif autoplay_lower == \"output\":\n",
				"\t\t\tdisplay(Audio(Path_Input_Final, autoplay = False))\n",
				"\t\t\tdisplay(Audio(Path_Output_Final, autoplay = True))\n",
				"\t\telif autoplay_lower == \"output after input\":\n",
				"\t\t\t# create Audio objects\n",
				"\t\t\tinput_audio = Audio(Path_Input_Final, autoplay = True)\n",
				"\t\t\toutput_audio = Audio(Path_Output_Final, autoplay = False)\n",
				"\n",
				"\t\t\t# use _repr_html_() to embed HTML\n",
				"\t\t\tinput_html = input_audio._repr_html_()\n",
				"\t\t\toutput_html = output_audio._repr_html_()\n",
				"\n",
				"\t\t\t# combine with JS for delayed output\n",
				"\t\t\thtml = f\"\"\"\n",
				"\t\t\t<div>\n",
				"\t\t\t\tInput:<br>{input_html}<br>\n",
				"\t\t\t\tOutput:<br>{output_html}\n",
				"\t\t\t</div>\n",
				"\t\t\t<script>\n",
				"\t\t\t\tvar input = document.querySelectorAll('audio')[document.querySelectorAll('audio').length-2];\n",
				"\t\t\t\tvar output = document.querySelectorAll('audio')[document.querySelectorAll('audio').length-1];\n",
				"\t\t\t\tinput.onended = () => {{\n",
				"\t\t\t\t\tsetTimeout(() => {{\n",
				"\t\t\t\t\t\toutput.play();\n",
				"\t\t\t\t\t}}, {int(gap * 1000)});\n",
				"\t\t\t\t}};\n",
				"\t\t\t</script>\n",
				"\t\t\t\"\"\"\n",
				"\t\t\tdisplay(HTML(html))\n",
				"\t\telse:\n",
				"\t\t\t# No autoplay\n",
				"\t\t\tdisplay(Audio(Path_Input_Final, autoplay = False))\n",
				"\t\t\tdisplay(Audio(Path_Output_Final, autoplay = False))\n",
				"\telse:\n",
				"\t\tprint(f'\"{Path_Output_Final}\" does not exist!')\n",
				"\n",
				"\tif File != Files_Final[-1]:\n",
				"\t\tprint()\n",
				"\n",
				"os.chdir(cwd)"
			],
			"metadata": {
				"cellView": "form",
				"id": "CIig2ZF55MBg"
			}
		},
		{
			"cell_type": "code",
			"metadata": {
				"id": "J43qejJ-2Tpp",
				"cellView": "form"
			},
			"source": [
				"#@title ### üîÆ <font class=\"markdown-google-sans\">Enable post-processing effects for inference</font>\n",
				"Post_Process = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_Post_Process\"] = str(Post_Process)\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### Pitch Shift\n",
				"#@markdown\n",
				"Pitch_Shift = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_Pitch_Shift\"] = str(Pitch_Shift)\n",
				"\n",
				"Pitch_Shift_Semitones = 0.0 #@param {type: \"slider\", min: -12.0, max: 12.0, step: 0.1}\n",
				"os.environ[\"INFER_Pitch_Shift_Semitones\"] = str(Pitch_Shift_Semitones)\n",
				"\n",
				"#@markdown ## Dynamics\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### Gain\n",
				"Gain = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_Gain\"] = str(Gain)\n",
				"\n",
				"#@markdown\n",
				"Gain_dB = 0.0 #@param {type: \"slider\", min: -20.0, max: 20.0, step: 0.1}\n",
				"os.environ[\"INFER_Gain_dB\"] = str(Gain_dB)\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### Distortion\n",
				"Distortion = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_Distortion\"] = str(Distortion)\n",
				"\n",
				"#@markdown\n",
				"Distortion_Gain = 0.0 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Distortion_Gain\"] = str(Distortion_Gain)\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### Clipping\n",
				"Clipping = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_Clipping\"] = str(Clipping)\n",
				"\n",
				"#@markdown\n",
				"Clipping_Threshold = 0.5 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Clipping_Threshold\"] = str(Clipping_Threshold)\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### Limiter\n",
				"Limiter = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_Limiter\"] = str(Limiter)\n",
				"\n",
				"Limiter_Threshold = -1.0 #@param {type: \"slider\", min: -20.0, max: 0.0, step: 0.1}\n",
				"os.environ[\"INFER_Limiter_Threshold\"] = str(Limiter_Threshold)\n",
				"\n",
				"Limiter_Release_Time = 0.05 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
				"os.environ[\"INFER_Limiter_Release_Time\"] = str(Limiter_Release_Time)\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### Compressor\n",
				"Compressor = False #@param{type: \"boolean\"}\n",
				"os.environ[\"INFER_Compressor\"] = str(Compressor)\n",
				"\n",
				"#@markdown\n",
				"Compressor_Threshold = -20.0 #@param {type: \"slider\", min: -60.0, max: 0.0, step: 0.1}\n",
				"os.environ[\"INFER_Compressor_Threshold\"] = str(Compressor_Threshold)\n",
				"\n",
				"Compressor_Ratio = 4.0 #@param {type: \"slider\", min: 1.0, max: 20.0, step: 0.1}\n",
				"os.environ[\"INFER_Compressor_Ratio\"] = str(Compressor_Ratio)\n",
				"\n",
				"Compressor_Attack = 0.001 #@param {type: \"slider\", min: 0.0, max: 0.1, step: 0.001}\n",
				"os.environ[\"INFER_Compressor_Attack\"] = str(Compressor_Attack)\n",
				"\n",
				"Compressor_Release = 0.1 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
				"os.environ[\"INFER_Compressor_Release\"] = str(Compressor_Release)\n",
				"\n",
				"#@markdown ## Effects\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### Reverb\n",
				"Reverb = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_Reverb\"] = str(Reverb)\n",
				"\n",
				"#@markdown\n",
				"Reverb_Room_Size = 0.5 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Reverb_Room_Size\"] = str(Reverb_Room_Size)\n",
				"\n",
				"Reverb_Damping = 0.5 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Reverb_Damping\"] = str(Reverb_Damping)\n",
				"\n",
				"Reverb_Wet_Gain = 0.0 #@param {type: \"slider\", min: -20.0, max: 20.0, step: 0.1}\n",
				"os.environ[\"INFER_Reverb_Wet_Gain\"] = str(Reverb_Wet_Gain)\n",
				"\n",
				"Reverb_Dry_Gain = 0.0 #@param {type: \"slider\", min: -20.0, max: 20.0, step: 0.1}\n",
				"os.environ[\"INFER_Reverb_Dry_Gain\"] = str(Reverb_Dry_Gain)\n",
				"\n",
				"Reverb_Width = 1.0 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Reverb_Width\"] = str(Reverb_Width)\n",
				"\n",
				"Reverb_Freeze_Mode = 0.0 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Reverb_Freeze_Mode\"] = str(Reverb_Freeze_Mode)\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### Delay\n",
				"Delay = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_Delay\"] = str(Delay)\n",
				"\n",
				"#@markdown\n",
				"Delay_Seconds = 0.1 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
				"os.environ[\"INFER_Delay_Seconds\"] = str(Delay_Seconds)\n",
				"\n",
				"Delay_Feedback = 0.5 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Delay_Feedback\"] = str(Delay_Feedback)\n",
				"\n",
				"Delay_Mix = 0.5 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Delay_Mix\"] = str(Delay_Mix)\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### Chorus\n",
				"Chorus = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_Chorus\"] = str(Chorus)\n",
				"\n",
				"#@markdown\n",
				"Chorus_Rate = 1.5 #@param {type: \"slider\", min: 0.1, max: 10.0, step: 0.1}\n",
				"os.environ[\"INFER_Chorus_Rate\"] = str(Chorus_Rate)\n",
				"\n",
				"Chorus_Depth = 0.1 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Chorus_Depth\"] = str(Chorus_Depth)\n",
				"\n",
				"Chorus_Center_Delay = 15.0 #@param {type: \"slider\", min: 0.0, max:50.0, step: 0.1}\n",
				"os.environ[\"INFER_Chorus_Center_Delay\"] = str(Chorus_Center_Delay)\n",
				"\n",
				"Chorus_Feedback = 0.25 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Chorus_Feedback\"] = str(Chorus_Feedback)\n",
				"\n",
				"Chorus_Mix = 0.5 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
				"os.environ[\"INFER_Chorus_Mix\"] = str(Chorus_Mix)\n",
				"\n",
				"#@markdown ---\n",
				"#@markdown ### BitCrush\n",
				"BitCrush = False #@param {type: \"boolean\"}\n",
				"os.environ[\"INFER_BitCrush\"] = str(BitCrush)\n",
				"\n",
				"#@markdown\n",
				"BitCrush_Bit_Depth = 4 #@param {type: \"slider\", min: 1, max: 16, step: 1}\n",
				"os.environ[\"INFER_BitCrush_Bit_Depth\"] = str(BitCrush_Bit_Depth)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "section_training"
			},
			"source": [
				"# <font class=\"markdown-google-sans\">üß† **Training**</font>"
			]
		},
		{
			"cell_type": "code",
			"metadata": {
				"id": "64V5TWxp05cn",
				"cellView": "form"
			},
			"source": [
				"#@title ### <font class=\"markdown-google-sans\">01. Setup Model Parameters</font>\n",
				"import warnings\n",
				"\n",
				"if not GPU:\n",
				"\twarnings.warn(GPU_Warning)\n",
				"\n",
				"Model_Name = \"Darwin\" #@param {type: \"string\"}\n",
				"Sample_Rate = \"48 000 Hz\" #@param [\"32 000 Hz\", \"40 000 Hz\", \"48 000 Hz\"] {allow-input: false}\n",
				"Sample_Rate_Display = Sample_Rate\n",
				"\n",
				"Sample_Rate = int(\"\".join((character for character in Sample_Rate if character.isdigit())))\n",
				"\n",
				"os.environ[\"Model_Name\"] = Model_Name\n",
				"os.environ[\"Sample_Rate\"] = str(Sample_Rate)"
			]
		},
		{
			"cell_type": "code",
			"metadata": {
				"id": "oBzqm4JkGGa0",
				"collapsed": true
			},
			"source": [
				"#@title ### <font class=\"markdown-google-sans\">02. Preprocess Dataset</font>\n",
				"import os\n",
				"import shutil\n",
				"import warnings\n",
				"\n",
				"if not GPU:\n",
				"\twarnings.warn(GPU_Warning)\n",
				"\n",
				"cwd = os.getcwd()\n",
				"os.chdir(Path_Code)\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"Path_Dataset = \"/content/drive/MyDrive/RVC/Data/Sets/Darwin\" #@param {type: \"string\"}\n",
				"\n",
				"Cut_Preprocess = \"Automatic\" #@param [\"Automatic\", \"Simple\", \"Skip\"]\n",
				"Chunk_Length = 3 #@param {type: \"slider\", min: 0.5, max: 5.0, step: 0.5}\n",
				"\n",
				"Process_Effects = False #@param {type: \"boolean\"}\n",
				"\n",
				"Noise_Reduction = False #@param {type: \"boolean\"}\n",
				"Noise_Reduction_Strength = 0.7 #@param {type: \"slider\", min: 0.0, max: 1.0, step:0.1}\n",
				"\n",
				"Overlap_Length = 0.3 #@param {type: \"slider\", min: 0, max: 0.5, step: 0.1}\n",
				"Normalization_Mode = \"None\" #@param [\"Pre\", \"Post\", \"None\"]\n",
				"Normalization_Mode = Normalization_Mode.lower()\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"os.environ[\"Path_Dataset\"] = Path_Dataset\n",
				"os.environ[\"Cut_Preprocess\"] = Cut_Preprocess\n",
				"os.environ[\"Chunk_Length\"] = str(Chunk_Length)\n",
				"os.environ[\"Overlap_Length\"] = str(Overlap_Length)\n",
				"os.environ[\"Process_Effects\"] = str(Process_Effects)\n",
				"os.environ[\"Noise_Reduction\"] = str(Noise_Reduction)\n",
				"os.environ[\"Noise_Reduction_Strength\"] = str(Noise_Reduction_Strength)\n",
				"os.environ[\"Normalization_Mode\"] = Normalization_Mode\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"Path_Logs_Current_Model = os.path.join(Path_Logs, Model_Name)\n",
				"\n",
				"if os.path.exists(Path_Logs_Current_Model):\n",
				"\tfor file in os.listdir(Path_Logs_Current_Model):\n",
				"\t\tobj = os.path.join(Path_Logs_Current_Model, file)\n",
				"\n",
				"\t\tif os.path.isdir(obj) and \"sliced_audios\" in file.lower():\n",
				"\t\t\tshutil.rmtree(obj, ignore_errors = True)\n",
				"\t\telif file.lower().startswith((\"model_info.json\")):\n",
				"\t\t\tos.remove(obj)\n",
				"\n",
				"\tprint(f'Removed sliced audio directories from \"logs{os.sep}{Model_Name}\" folder')\n",
				"\tprint()\n",
				"\n",
				"!echo python \"core.py\" \"preprocess\" \\\n",
				"\t--model_name \"$Model_Name\" \\\n",
				"\t--dataset_path \"$Path_Dataset\" \\\n",
				"\t--sample_rate \"$Sample_Rate\" \\\n",
				"\t--cpu_cores \"$CPU_Cores\" \\\n",
				"\t--cut_preprocess \"$Cut_Preprocess\" \\\n",
				"\t--process_effects \"$Process_Effects\" \\\n",
				"\t--noise_reduction \"$Noise_Reduction\" \\\n",
				"\t--noise_reduction_strength \"$Noise_Reduction_Strength\" \\\n",
				"\t--chunk_len \"$Chunk_Length\" \\\n",
				"\t--overlap_len \"$Overlap_Length\" \\\n",
				"\t--normalization_mode \"$Normalization_Mode\"\n",
				"\n",
				"print()\n",
				"\n",
				"!python \"core.py\" \"preprocess\" \\\n",
				"\t--model_name \"$Model_Name\" \\\n",
				"\t--dataset_path \"$Path_Dataset\" \\\n",
				"\t--sample_rate \"$Sample_Rate\" \\\n",
				"\t--cpu_cores \"$CPU_Cores\" \\\n",
				"\t--cut_preprocess \"$Cut_Preprocess\" \\\n",
				"\t--process_effects \"$Process_Effects\" \\\n",
				"\t--noise_reduction \"$Noise_Reduction\" \\\n",
				"\t--noise_reduction_strength \"$Noise_Reduction_Strength\" \\\n",
				"\t--chunk_len \"$Chunk_Length\" \\\n",
				"\t--overlap_len \"$Overlap_Length\" \\\n",
				"\t--normalization_mode \"$Normalization_Mode\"\n",
				"\n",
				"os.chdir(cwd)"
			]
		},
		{
			"cell_type": "code",
			"metadata": {
				"cellView": "form",
				"id": "zWMiMYfRJTJv"
			},
			"source": [
				"#@title ### <font class=\"markdown-google-sans\">03. Extract Features</font>\n",
				"import os\n",
				"import warnings\n",
				"\n",
				"if not GPU:\n",
				"    warnings.warn(GPU_Warning)\n",
				"\n",
				"cwd = os.getcwd()\n",
				"os.chdir(Path_Code)\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"F0_Method = \"RMVPE\" # @param [\"Crepe\", \"Crepe Tiny\", \"RMVPE\"] {allow-input: false}\n",
				"F0_Method = F0_Method.lower().replace(\" \", \"-\")\n",
				"\n",
				"Embedder_Model = \"ContentVec\" # @param [\"ContentVec\", \"Spin v2\", \"Chinese HuBERT Base\", \"Japanese HuBERT Base\", \"Korean HuBERT Base\", \"Custom\"] {allow-input: false}\n",
				"Embedder_Model = Embedder_Model.lower().replace(\" \", \"-\")\n",
				"Embedder_Model_Custom = \"\" #@param {type: \"string\"}\n",
				"\n",
				"Mutes_Amount = 2 #@param {type: \"slider\", min: 0, max: 10, step: 1}\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"if not os.path.exists(\n",
				"    os.path.join(Path_Logs_Current_Model, \"sliced_audios\")\n",
				"):\n",
				"    raise Exception(\"No directory with sliced audios has been found, please run the feature extraction cell\")\n",
				"\n",
				"if os.path.exists(Path_Logs_Current_Model):\n",
				"\tfor file in os.listdir(Path_Logs_Current_Model):\n",
				"\t\tobj = os.path.join(Path_Logs_Current_Model, file)\n",
				"\n",
				"\t\tif os.path.isdir(obj):\n",
				"\t\t\tif file.lower().startswith((\"extracted\", \"f0\")):\n",
				"\t\t\t\tshutil.rmtree(obj, ignore_errors = True)\n",
				"\t\telif file.lower().startswith((\"config.json\", \"filelist.txt\")):\n",
				"\t\t\tos.remove(obj)\n",
				"\n",
				"\tprint(f'Removed extraction directories from \"logs{os.sep}{Model_Name}\" folder')\n",
				"\tprint()\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"!echo python \"core.py\" \"extract\" \\\n",
				"    --model_name \"{Model_Name}\" \\\n",
				"    --f0_method \"{F0_Method}\" \\\n",
				"    --sample_rate \"{Sample_Rate}\" \\\n",
				"    --cpu_cores \"{CPU_Cores}\" \\\n",
				"    --gpu \"0\" \\\n",
				"    --embedder_model \"{Embedder_Model}\" \\\n",
				"    --embedder_model_custom \"{Embedder_Model_Custom}\" \\\n",
				"    --include_mutes \"{Mutes_Amount}\"\n",
				"\n",
				"print()\n",
				"\n",
				"!python \"core.py\" \"extract\" \\\n",
				"    --model_name \"{Model_Name}\" \\\n",
				"    --f0_method \"{F0_Method}\" \\\n",
				"    --sample_rate \"{Sample_Rate}\" \\\n",
				"    --cpu_cores \"{CPU_Cores}\" \\\n",
				"    --gpu \"0\" \\\n",
				"    --embedder_model \"{Embedder_Model}\" \\\n",
				"    --embedder_model_custom \"{Embedder_Model_Custom}\" \\\n",
				"    --include_mutes \"{Mutes_Amount}\"\n",
				"\n",
				"os.chdir(cwd)"
			]
		},
		{
			"cell_type": "code",
			"metadata": {
				"id": "bHLs5AT4Q1ck"
			},
			"source": [
				"#@title ### <font class=\"markdown-google-sans\">04. Generate Index file</font>\n",
				"#@markdown > <font color=red>**GPU required.**</font>\n",
				"import os\n",
				"\n",
				"if not GPU:\n",
				"\traise Exception(GPU_Error)\n",
				"\n",
				"cwd = os.getcwd()\n",
				"os.chdir(Path_Code)\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"Algorithm = \"Auto\" #@param [\"Auto\", \"Faiss\", \"KMeans\"] {allow-input: false}\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"if os.path.exists(Path_Logs_Current_Model):\n",
				"\tfor file in os.listdir(Path_Logs_Current_Model):\n",
				"\t\tobj = os.path.join(Path_Logs_Current_Model, file)\n",
				"\n",
				"\t\tif os.path.isfile(obj) and os.path.splitext(file.lower())[-1] == \".index\":\n",
				"\t\t\tprint(f'Removed first found \".index\" file inside \"logs\" directory (\"{Model_Name}.index\")')\n",
				"\t\t\tos.remove(obj)\n",
				"\t\t\tbreak\n",
				"\n",
				"\tprint()\n",
				"\n",
				"!python \"core.py\" \"index\" \\\n",
				"\t--model_name \"{Model_Name}\" \\\n",
				"\t--index_algorithm \"{Algorithm}\"\n",
				"\n",
				"os.chdir(cwd)"
			]
		},
		{
			"cell_type": "code",
			"metadata": {
				"id": "yNjSvyQiOe7R",
				"cellView": "form"
			},
			"source": [
				"#@title ### <font class=\"markdown-google-sans\">05. Download Pretraineds</font>\n",
				"#@markdown > Downloads the `G`enerator/`D`enominator files and generates the paths needed for the `Start Training` section.\n",
				"import os\n",
				"import re\n",
				"import requests\n",
				"from tqdm import tqdm\n",
				"\n",
				"#@markdown ---\n",
				"\n",
				"#@markdown üß† Uses smart auto-fill mechanics:\n",
				"#@markdown <details>\n",
				"#@markdown \t<summary>Tries to find <code>G</code>/<code>D</code> automatically based on <code>Sample_Rate</code> if any of these fields are repository URL </summary>\n",
				"#@markdown \t<ul>\n",
				"#@markdown \t\t<li>From:\n",
				"#@markdown \t\t\t<ul>\n",
				"#@markdown \t\t\t\t<li><code>G</code>: <code>hf.co/user/repo</code></li>\n",
				"#@markdown \t\t\t\t<li><code>D</code>: <code>&ensp;</code></li>\n",
				"#@markdown \t\t\t</ul>\n",
				"#@markdown \t\t</li>\n",
				"#@markdown \t\t<li>To:\n",
				"#@markdown \t\t\t<ul>\n",
				"#@markdown \t\t\t\t<li><code>G</code>: <code>hf.co/user/repo/resolve/main/G_40k.pth</code></li>\n",
				"#@markdown \t\t\t\t<li><code>D</code>: <code>hf.co/user/repo/resolve/main/D_40k.pth</code></li>\n",
				"#@markdown \t\t\t</ul>\n",
				"#@markdown \t\t</li>\n",
				"#@markdown \t\t<br>Throws an error if any URL is not found.\n",
				"#@markdown \t</ul>\n",
				"#@markdown </details>\n",
				"#@markdown <details>\n",
				"#@markdown \t<summary>If one of links is empty, it attempts to retrieve </code>G</code>/</code>D</code> from each other </summary>\n",
				"#@markdown \t<ul>\n",
				"#@markdown \t\t<li>From:\n",
				"#@markdown \t\t\t<ul>\n",
				"#@markdown \t\t\t\t<li><code>G</code>: <code>&ensp;</code></li>\n",
				"#@markdown \t\t\t\t<li><code>D</code>: <code>hf.co/user/repo/resolve/main/D_48k.pth</code></li>\n",
				"#@markdown \t\t\t</ul>\n",
				"#@markdown \t\t</li>\n",
				"#@markdown \t\t<li>To:\n",
				"#@markdown \t\t\t<ul>\n",
				"#@markdown \t\t\t\t<li><code>G</code>: <code>hf.co/user/repo/resolve/main/G_48k.pth</code></li>\n",
				"#@markdown \t\t\t\t<li><code>D</code>: <code>hf.co/user/repo/resolve/main/D_48k.pth</code></li>\n",
				"#@markdown \t\t\t</ul>\n",
				"#@markdown \t\t</li>\n",
				"#@markdown \t\t<br>Throws an error if any URL is not found.\n",
				"#@markdown \t</ul>\n",
				"#@markdown </details>\n",
				"#@markdown <details>\n",
				"#@markdown \t<summary>If no valid sample rate is found in <code>G</code>/<code>D</code> file base names, it tries to parse the one based on established <code>Sample_Rate</code> variable</summary>\n",
				"#@markdown \t<ul>\n",
				"#@markdown \t\t<li>From:\n",
				"#@markdown \t\t\t<ul>\n",
				"#@markdown \t\t\t\t<li><code>G</code>: <code>hf.co/user/repo/resolve/main/G_32k.pth</code></li>\n",
				"#@markdown \t\t\t\t<li><code>D</code>: <code>&ensp;</code></li>\n",
				"#@markdown \t\t\t</ul>\n",
				"#@markdown \t\t</li>\n",
				"#@markdown \t\t<li>To:\n",
				"#@markdown \t\t\t<ul>\n",
				"#@markdown \t\t\t\t<li><code>G</code>: <code>hf.co/user/repo/resolve/main/G_40k.pth</code></li>\n",
				"#@markdown \t\t\t\t<li><code>D</code>: <code>hf.co/user/repo/resolve/main/D_40k.pth</code></li>\n",
				"#@markdown \t\t\t</ul>\n",
				"#@markdown \t\t</li>\n",
				"#@markdown \t\t<br>Throws an error if any URL is not found.\n",
				"#@markdown \t</ul>\n",
				"#@markdown </details>\n",
				"\n",
				"#@markdown ---\n",
				"\n",
				"G = \"http://hf.co/repo/user\" #@param {type: \"string\"}\n",
				"D = \"\" #@param {type: \"string\"}\n",
				"\n",
				"Directory_Pretraineds_Destination = \"/content/Applio/rvc/models/pretraineds/custom\"\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"#@markdown ---\n",
				"\n",
				"#@markdown > Downloads the checkpoint again, or if it doesn't exist, downloads other one with highest samplerate found.\n",
				"#@markdown >\n",
				"#@markdown > <font color=gold>‚ö†Ô∏è **If different samplerate checkpoints will be downloaded, the preprocessing and feature extraction cells have to be re-ran.**</font>\n",
				"Forced = False #@param {type: \"boolean\"}\n",
				"\n",
				"#@markdown ---\n",
				"\n",
				"Pattern_Prefixes = \"{GD}_, {GD}, _{GD}\" #@param {type: \"string\"}\n",
				"Pattern_Samplerate = \"32k, 32000, 32, 40k, 40000, 40, 48k, 48000, 48\" #@param {type: \"string\"}\n",
				"\n",
				"Pattern_Prefixes = [p.strip() for p in Pattern_Prefixes.split(\",\") if p]\n",
				"Pattern_Samplerate = [p.strip() for p in Pattern_Samplerate.split(\",\") if p]\n",
				"\n",
				"def samplerate_to_number(n: str) -> int:\n",
				"\tsamplerate = n.lower().replace(\"k\", \"000\")\n",
				"\treturn int(samplerate)\n",
				"\n",
				"Pattern_Samplerate = sorted(Pattern_Samplerate, key = samplerate_to_number, reverse = True)\n",
				"\n",
				"def url_exists(url: str) -> bool:\n",
				"\ttry:\n",
				"\t\tr = requests.get(\n",
				"\t\t\turl,\n",
				"\t\t\tstream = True,\n",
				"\t\t\theaders = {\"Range\": \"bytes=0-0\"},\n",
				"\t\t\ttimeout = 10,\n",
				"\t\t\tallow_redirects = True\n",
				"\t\t)\n",
				"\t\treturn r.status_code in (200, 206)\n",
				"\texcept:\n",
				"\t\treturn False\n",
				"\n",
				"def extract_samplerate(text: str, patterns_number: list):\n",
				"\tpatterns = \"|\".join(patterns_number)\n",
				"\tmatch = re.search(rf\"({patterns})\", text, re.IGNORECASE)\n",
				"\treturn match.group(1) if match else None\n",
				"\n",
				"def normalize_repo_url(url: str) -> str:\n",
				"\treturn url.rstrip(\"/\")\n",
				"\n",
				"def build_candidates(\n",
				"\trepo_url: str, target: str,\n",
				"\tpatterns_string: list, patterns_number: list\n",
				"):\n",
				"\tbase = normalize_repo_url(repo_url)\n",
				"\tcandidates = []\n",
				"\n",
				"\tfor sr in patterns_number:\n",
				"\t\tfor prefix in patterns_string:\n",
				"\t\t\tname = f\"{prefix.format(GD = target)}{sr}.pth\"\n",
				"\t\t\tcandidates.append(f\"{base}/resolve/main/{name}\")\n",
				"\n",
				"\treturn candidates\n",
				"\n",
				"def infer_from_repo(\n",
				"\trepo_url: str, target: str,\n",
				"\tpatterns_string: list, patterns_number: list\n",
				") -> str:\n",
				"\tfor candidate in build_candidates(repo_url, target, patterns_string, patterns_number):\n",
				"\t\tif url_exists(candidate):\n",
				"\t\t\treturn candidate\n",
				"\n",
				"\traise FileNotFoundError(\n",
				"\t\tf\"No valid {target} URL found in repo using supported patterns.\"\n",
				"\t)\n",
				"\n",
				"def infer_from_other(\n",
				"\tother_url: str, want: str,\n",
				"\tpatterns_string: list, patterns_number: list\n",
				") -> str:\n",
				"\t# extract samplerate from the other URL\n",
				"\tsamplerate = extract_samplerate(other_url, patterns_number)\n",
				"\tif not samplerate:\n",
				"\t\tsamplerate = f\"{int(Sample_Rate) // 1000}k\"\n",
				"\n",
				"\t# build candidates replace the prefix only\n",
				"\tcandidates = []\n",
				"\tfor prefix in patterns_string:\n",
				"\t\t# remove old prefix and samplerate\n",
				"\t\tname = f\"{prefix.format(GD = want)}{samplerate}.pth\"\n",
				"\t\tcandidate = re.sub(r\"([^/]+\\.pth)$\", name, other_url)\n",
				"\t\tcandidates.append(candidate)\n",
				"\n",
				"\t# pick the first one that exists\n",
				"\tfor c in candidates:\n",
				"\t\tif url_exists(c):\n",
				"\t\t\treturn c\n",
				"\n",
				"\traise FileNotFoundError(f\"Could not infer {want} from {other_url} using supported patterns.\")\n",
				"\n",
				"#-=-=-=-#\n",
				"# Auto fill\n",
				"\n",
				"if not all((G, D)):\n",
				"\tprint(\"Sample rate:\", Sample_Rate_Display)\n",
				"\tprint()\n",
				"\n",
				"if G and not G.endswith(\".pth\"):\n",
				"\tG = infer_from_repo(\n",
				"\t\tG, \"G\",\n",
				"\t\tPattern_Prefixes, Pattern_Samplerate\n",
				"\t)\n",
				"\n",
				"if D and not D.endswith(\".pth\"):\n",
				"\tD = infer_from_repo(\n",
				"\t\tD, \"D\",\n",
				"\t\tPattern_Prefixes, Pattern_Samplerate\n",
				"\t)\n",
				"\n",
				"if not G and D:\n",
				"\tG = infer_from_other(\n",
				"\t\tD, \"G\",\n",
				"\t\tPattern_Prefixes, Pattern_Samplerate\n",
				"\t)\n",
				"\n",
				"if not D and G:\n",
				"\tD = infer_from_other(\n",
				"\t\tG, \"D\",\n",
				"\t\tPattern_Prefixes, Pattern_Samplerate\n",
				"\t)\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"def download(url: str, destination: str, label: str, forced: bool):\n",
				"\tif not url:\n",
				"\t\treturn None\n",
				"\n",
				"\tos.makedirs(destination, exist_ok = True)\n",
				"\tfilename = os.path.basename(url)\n",
				"\tdest_path = os.path.join(destination, filename)\n",
				"\n",
				"\tif not forced and os.path.exists(dest_path):\n",
				"\t\tprint(f'\"{label}\" already exists.')\n",
				"\t\treturn dest_path\n",
				"\n",
				"\tprint(f'Downloading \"{label}\": {filename}...')\n",
				"\n",
				"\tresponse = requests.get(url, stream=True)\n",
				"\tresponse.raise_for_status()\n",
				"\ttotal_size = int(response.headers.get(\"content-length\", 0))\n",
				"\n",
				"\twith open(dest_path, \"wb\") as f, tqdm(\n",
				"\t\ttotal = total_size,\n",
				"\t\tunit = \"iB\",\n",
				"\t\tunit_scale = True,\n",
				"\t\tunit_divisor = 1024\n",
				"\t) as bar:\n",
				"\t\tfor chunk in response.iter_content(chunk_size = 8192):\n",
				"\t\t\tif chunk:\n",
				"\t\t\t\tbar.update(f.write(chunk))\n",
				"\n",
				"\treturn dest_path\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"Path_G = download(G, Directory_Pretraineds_Destination, \"G\", Forced)\n",
				"Path_D = download(D, Directory_Pretraineds_Destination, \"D\", Forced)\n",
				"\n",
				"if Forced:\n",
				"\tprint()\n",
				"\tprint()\n",
				"else:\n",
				"\tprint()\n",
				"\n",
				"print(f\"G Path: {Path_G}\" if Path_G else \"No G model downloaded.\")\n",
				"print(f\"D Path: {Path_D}\" if Path_D else \"No D model downloaded.\")"
			]
		},
		{
			"cell_type": "code",
			"metadata": {
				"cellView": "form",
				"id": "TI6LLdIzKAIa"
			},
			"source": [
				"#@title ### 06. <font class=\"markdown-google-sans\">Start</title>\n",
				"import os\n",
				"from pathlib import Path\n",
				"\n",
				"cwd = os.getcwd()\n",
				"os.chdir(Path_Code)\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"#@markdown #### ‚öôÔ∏è Settings\n",
				"Total_Epoch = 500  #@param {type: \"integer\"}\n",
				"Batch_Size = 8  #@param {type: \"slider\", min: 1, max: 25, step: 0}\n",
				"Vocoder = \"HiFi-GAN\" #@param [\"HiFi-GAN\", \"RefineGAN\"]\n",
				"Launch_Tensorboard = False # @param{type: \"boolean\"}\n",
				"\n",
				"#@markdown #### ‚ùì Optional\n",
				"#@markdown In case you select custom pretrained, you will have to download the pretraineds and enter their paths.\n",
				"Custom_Pretrain = True #@param {type: \"boolean\"}\n",
				"G = \"/content/Applio/rvc/models/pretraineds/custom/G_48k.pth\" #@param {type: \"string\"}\n",
				"D = \"/content/Applio/rvc/models/pretraineds/custom/D_48k.pth\" #@param {type: \"string\"}\n",
				"\n",
				"#@markdown ### ‚û°Ô∏è Choose how many epochs your model will be stored\n",
				"Save_Every_Epoch = 10 # @param {type: \"slider\", min: 1, max: 100, step: 0}\n",
				"Save_Only_Latest = True # @param{type: \"boolean\"}\n",
				"Save_Every_Weights = True # @param{type: \"boolean\"}\n",
				"\n",
				"#@markdown ---\n",
				"\n",
				"#@markdown #### Overtraining\n",
				"Detector = True # @param {type: \"boolean\"}\n",
				"Threshold = 50 # @param {type: \"slider\", min: 1, max: 100, step: 0}\n",
				"\n",
				"#@markdown ---\n",
				"\n",
				"Pretrained = True #@param{type: \"boolean\"}\n",
				"Cleanup = False #@param{type: \"boolean\"}\n",
				"Cache_Data_In_GPU = True #@param {type: \"boolean\"}\n",
				"Checkpointing = False\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"if Launch_Tensorboard:\n",
				"\t%load_ext tensorboard\n",
				"\t%tensorboard --logdir logs --bind_all\n",
				"\n",
				"!python \"core.py\" train \\\n",
				"\t--model_name \"{Model_Name}\" \\\n",
				"\t--save_every_epoch \"{Save_Every_Epoch}\" \\\n",
				"\t--save_only_latest \"{Save_Only_Latest}\" \\\n",
				"\t--save_every_weights \"{Save_Every_Weights}\" \\\n",
				"\t--total_epoch \"{Total_Epoch}\" \\\n",
				"\t--sample_rate \"{Sample_Rate}\" \\\n",
				"\t--batch_size \"{Batch_Size}\" \\\n",
				"\t--gpu 0 \\\n",
				"\t--pretrained \"{Pretrained}\" \\\n",
				"\t--custom_pretrained \"{Custom_Pretrain}\" \\\n",
				"\t--g_pretrained_path \"{G}\" \\\n",
				"\t--d_pretrained_path \"{D}\" \\\n",
				"\t--overtraining_detector \"{Detector}\" \\\n",
				"\t--overtraining_threshold \"{Threshold}\" \\\n",
				"\t--cleanup \"{Cleanup}\" \\\n",
				"\t--cache_data_in_gpu \"{Cache_Data_In_GPU}\" \\\n",
				"\t--vocoder \"{Vocoder}\" \\\n",
				"\t--checkpointing \"{Checkpointing}\"\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"print(\"\\n\" * 3)\n",
				"\n",
				"Files_Current_Model_Root_CKPT = Path(Path_Logs_Current_Model).glob(\"*.pth\")\n",
				"Latest = max(Files_Current_Model_Root_CKPT, key = lambda p: p.stat().st_mtime, default = None)\n",
				"\n",
				"if Latest:\n",
				"\tprint(\"Probable output file:\".upper())\n",
				"\tprint(Latest.resolve())\n",
				"else:\n",
				"\tprint(\"No .pth file found.\")\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"os.chdir(cwd)"
			]
		},
		{
			"cell_type": "markdown",
			"source": [
				"## üö¢ <font class=\"markdown-google-sans\">Export-related</font>"
			],
			"metadata": {
				"id": "section_inference_export"
			}
		},
		{
			"cell_type": "code",
			"source": [
				"#@title ### üìâ <font class=\"markdown-google-sans\">Optimize model</font>\n",
				"#@markdown > Recompresses a model archive for inference by removing unused files and repacking with max compression.\n",
				"#@markdown >\n",
				"#@markdown > <font color=gold>‚ö†Ô∏è **Redundant for `Training` export mode.**</font>\n",
				"import os\n",
				"import shutil\n",
				"import zipfile\n",
				"import tempfile\n",
				"import subprocess\n",
				"from pathlib import Path\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"Packer = \"7-Zip\" #@param [\"ZipFile\", \"7-Zip\"]\n",
				"use_7z = Format.lower().startswith(\"7\")\n",
				"\n",
				"DEBUG = True\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"def debug(msg):\n",
				"\tif DEBUG:\n",
				"\t\tprint(f\"[DEBUG] {msg}\")\n",
				"\n",
				"# detect 7z binary (Colab + local safe)\n",
				"PATH_PACKAGE_7Z = None\n",
				"\n",
				"for name in (\"7z\", \"7zz\"):\n",
				"\tfound = shutil.which(name)\n",
				"\tif found:\n",
				"\t\tPATH_PACKAGE_7Z = found\n",
				"\t\tbreak\n",
				"\n",
				"debug(f\"7z binary: {PATH_PACKAGE_7Z}\")\n",
				"\n",
				"# compressor\n",
				"class ArchiveCompressor:\n",
				"\tdef __init__(self, destination: str, directory: str):\n",
				"\t\tself.destination = destination\n",
				"\t\tself.directory = directory\n",
				"\t\tassert os.path.exists(directory)\n",
				"\n",
				"\tdef SevenZip(self, binary: str):\n",
				"\t\tif not binary:\n",
				"\t\t\treturn False\n",
				"\n",
				"\t\tdebug(\"Using 7z CLI compression\")\n",
				"\n",
				"\t\ttry:\n",
				"\t\t\tsubprocess.run(\n",
				"\t\t\t\t[binary, \"a\", \"-tzip\", \"-mx=9\", self.destination, self.directory],\n",
				"\t\t\t\tcheck = True,\n",
				"\t\t\t\tstdout=subprocess.DEVNULL,\n",
				"\t\t\t\tstderr=subprocess.DEVNULL,\n",
				"\t\t\t)\n",
				"\t\t\treturn self.destination\n",
				"\t\texcept subprocess.CalledProcessError as e:\n",
				"\t\t\tdebug(f\"7z failed: {e}\")\n",
				"\t\t\treturn False\n",
				"\n",
				"\tdef Zip(self):\n",
				"\t\tdebug(\"Using Python ZipFile fallback\")\n",
				"\n",
				"\t\tbase_dir = Path(self.directory)\n",
				"\n",
				"\t\twith zipfile.ZipFile(\n",
				"\t\t\tself.destination,\n",
				"\t\t\t\"w\",\n",
				"\t\t\tcompression = zipfile.ZIP_DEFLATED,\n",
				"\t\t\tcompresslevel = 9,\n",
				"\t\t) as zip_out:\n",
				"\t\t\tfor file_path in base_dir.rglob(\"*\"):\n",
				"\t\t\t\tif file_path.is_file():\n",
				"\t\t\t\t\trel_path = file_path.relative_to(base_dir.parent)\n",
				"\t\t\t\t\tzip_out.write(file_path, arcname=rel_path)\n",
				"\n",
				"\t\treturn self.destination\n",
				"\n",
				"# optimize model\n",
				"def optimize_model(path: str, package_7z: str | None):\n",
				"\tif not zipfile.is_zipfile(path):\n",
				"\t\traise ValueError(\"Input file is not a ZIP archive\")\n",
				"\n",
				"\twith tempfile.TemporaryDirectory() as tmpdir:\n",
				"\t\tsize_in = os.path.getsize(path)\n",
				"\t\tdebug(f\"Original size: {size_in / 1024 / 1024:.2f} MB\")\n",
				"\n",
				"\t\twith zipfile.ZipFile(path, \"r\") as zip_in:\n",
				"\t\t\tzip_in.extractall(tmpdir)\n",
				"\n",
				"\t\t# expect exactly one top-level directory\n",
				"\t\troot_items = os.listdir(tmpdir)\n",
				"\t\tif len(root_items) != 1:\n",
				"\t\t\traise ValueError(\"Expected a single top-level directory\")\n",
				"\n",
				"\t\ttop_dir = os.path.join(tmpdir, root_items[0])\n",
				"\t\ttmp_output = os.path.join(tmpdir, \"._compressed.zip\")\n",
				"\n",
				"\t\t# cleanup rules\n",
				"\t\tbyteorder_path = os.path.join(top_dir, \"byteorder\")\n",
				"\t\tif os.path.isfile(byteorder_path):\n",
				"\t\t\tdebug(\"Removing byteorder\")\n",
				"\t\t\tos.remove(byteorder_path)\n",
				"\n",
				"\t\tdata_dir = os.path.join(top_dir, \".data\")\n",
				"\t\tif os.path.isdir(data_dir):\n",
				"\t\t\tdebug(\"Removing .data directory\")\n",
				"\t\t\tshutil.rmtree(data_dir)\n",
				"\n",
				"\t\tversion_path = os.path.join(top_dir, \"version\")\n",
				"\t\tif os.path.isfile(version_path):\n",
				"\t\t\twith open(version_path, \"r\") as vf:\n",
				"\t\t\t\tcontent = vf.read().rstrip(\"\\n\")\n",
				"\t\t\twith open(version_path, \"w\") as vf:\n",
				"\t\t\t\tvf.write(content)\n",
				"\n",
				"\t\t# compress\n",
				"\t\tarchive_compressor = ArchiveCompressor(tmp_output, top_dir)\n",
				"\n",
				"\t\tsuccess = False\n",
				"\t\tif use_7z and package_7z:\n",
				"\t\t\tsuccess = archive_compressor.SevenZip(package_7z)\n",
				"\n",
				"\t\tif not success:\n",
				"\t\t\tprint(\"[Fallback] 7z unavailable or failed, using ZipFile\")\n",
				"\t\t\tarchive_compressor.Zip()\n",
				"\n",
				"\t\tsize_out = os.path.getsize(tmp_output)\n",
				"\t\tdebug(f\"Optimized size: {size_out / 1024 / 1024:.2f} MB\")\n",
				"\n",
				"\t\tshutil.move(tmp_output, path)\n",
				"\n",
				"\treturn path, size_in, size_out\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"if not Latest:\n",
				"\twarnings.warn(\"Model not found, ignoring\")\n",
				"\n",
				"optimized_path, before, after = optimize_model(\n",
				"\tLatest,\n",
				"\tPATH_PACKAGE_7Z if use_7z else None,\n",
				")\n",
				"\n",
				"print(\"\\n‚úÖ Optimization complete\")\n",
				"print(f\"{'üì¶ Before:':<10} {before / 1024 / 1024:>8.2f} MB\")\n",
				"print(f\"{'üìâ After:':<10} {after / 1024 / 1024:>8.2f} MB\")\n",
				"print(f\"{'üíæ Saved:':<10} {(before - after) / 1024 / 1024:>8.2f} MB\")"
			],
			"metadata": {
				"cellView": "form",
				"id": "K_ud9qfEpo6-"
			}
		},
		{
			"cell_type": "code",
			"metadata": {
				"id": "X_eU_SoiHIQg",
				"cellView": "form"
			},
			"source": [
				"#@title ### üì§ <font style=\"markdown-google-sans\">Export model</font>\n",
				"#@markdown - Training: Bigger file size, can continue training\n",
				"#@markdown - Inference: Smaller file size, only for model inference\n",
				"import os\n",
				"import shutil\n",
				"import zipfile\n",
				"import warnings\n",
				"import subprocess\n",
				"from google.colab import files\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"#@markdown > ‚ö†Ô∏è <font color=red>**`7Z` is unsupported in this notebook's inference.**</font>\n",
				"Format = \"ZIP\" #@param [\"ZIP\", \"7Z\"]\n",
				"\n",
				"#@markdown > Exporting for training is only recommended for use outside of Applio, if you plan to resume training later, use [Sync with Google Drive](#scrollTo=section_inference_syncdrive) cell instead.\n",
				"Mode = \"Inference\" #@param [\"Inference\", \"Training\"]\n",
				"\n",
				"#-=-=-=-#\n",
				"\n",
				"DEBUG = True\n",
				"\n",
				"def debug(msg):\n",
				"\tif DEBUG:\n",
				"\t\tprint(f\"[DEBUG] {msg}\")\n",
				"\n",
				"# Detect 7z availability\n",
				"Format = Format.upper()\n",
				"sevenzip_mode = \"zip\"\n",
				"\n",
				"if Format == \"7Z\":\n",
				"\tdebug(\"Requested format: 7Z\")\n",
				"\n",
				"\t# Try py7zr\n",
				"\ttry:\n",
				"\t\timport py7zr\n",
				"\t\tsevenzip_mode = \"py7zr\"\n",
				"\t\tdebug(\"Using py7zr backend\")\n",
				"\texcept Exception as e:\n",
				"\t\tdebug(f\"py7zr not available: {e}\")\n",
				"\n",
				"\t\t# Try CLI 7z (NON-BLOCKING CHECK)\n",
				"\t\ttry:\n",
				"\t\t\tsubprocess.run(\n",
				"\t\t\t\t[\"7z\", \"--help\"],\n",
				"\t\t\t\tstdout = subprocess.DEVNULL,\n",
				"\t\t\t\tstderr = subprocess.DEVNULL,\n",
				"\t\t\t\ttimeout = 5,\n",
				"\t\t\t\tcheck = False,\n",
				"\t\t\t)\n",
				"\t\t\tsevenzip_mode = \"cli\"\n",
				"\t\t\tdebug(\"Using 7z CLI backend\")\n",
				"\t\texcept Exception as e:\n",
				"\t\t\twarnings.warn(f\"7z CLI unavailable ({e}), falling back to ZIP\")\n",
				"\t\t\tsevenzip_mode = \"zip\"\n",
				"\n",
				"else:\n",
				"\tdebug(\"Requested format: ZIP\")\n",
				"\n",
				"# Paths\n",
				"Path_Export = os.path.join(Path_Drive_Full, Title)\n",
				"if not Use_Legacy_Backup_Directory_Names:\n",
				"\tPath_Export = os.path.join(Path_Export, \"Exported\")\n",
				"\n",
				"logs_folder = Path_Logs_Current_Model\n",
				"if not os.path.isdir(Path_Logs_Current_Model):\n",
				"\traise FileNotFoundError(f\"{Model_Name} model folder not found\")\n",
				"\n",
				"model_root = os.path.dirname(Path_Logs_Current_Model)\n",
				"model_dir = os.path.join(model_root, Model_Name)\n",
				"\n",
				"ext = \".7z\" if sevenzip_mode in (\"py7zr\", \"cli\") else \".zip\"\n",
				"archive_path = os.path.join(os.getcwd(), f\"{Model_Name}{ext}\")\n",
				"\n",
				"debug(f\"Archive path: {archive_path}\")\n",
				"debug(f\"Compression backend: {sevenzip_mode}\")\n",
				"\n",
				"# Helpers\n",
				"def pack_directory(src_dir, archive_file):\n",
				"\tdebug(f\"Packing directory: {src_dir}\")\n",
				"\n",
				"\tif sevenzip_mode == \"py7zr\":\n",
				"\t\twith py7zr.SevenZipFile(archive_file, \"w\") as zf:\n",
				"\t\t\tzf.writeall(src_dir, arcname = os.path.basename(src_dir))\n",
				"\n",
				"\telif sevenzip_mode == \"cli\":\n",
				"\t\tsubprocess.check_call([\n",
				"\t\t\t\"7z\", \"a\", \"-t7z\", archive_file, src_dir\n",
				"\t\t])\n",
				"\n",
				"\telse:\n",
				"\t\twith zipfile.ZipFile(archive_file, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
				"\t\t\tfor root, _, files in os.walk(src_dir):\n",
				"\t\t\t\tfor name in files:\n",
				"\t\t\t\t\tfull_path = os.path.join(root, name)\n",
				"\t\t\t\t\tarcname = os.path.relpath(full_path, os.path.dirname(src_dir))\n",
				"\t\t\t\t\tzf.write(full_path, arcname)\n",
				"\n",
				"def pack_files(files, archive_file):\n",
				"\tdebug(f\"Packing files: {files}\")\n",
				"\n",
				"\tif sevenzip_mode == \"py7zr\":\n",
				"\t\twith py7zr.SevenZipFile(archive_file, \"w\") as zf:\n",
				"\t\t\tfor f in files:\n",
				"\t\t\t\tif os.path.isfile(f):\n",
				"\t\t\t\t\tzf.write(f, arcname = os.path.relpath(f, model_root))\n",
				"\telif sevenzip_mode == \"cli\":\n",
				"\t\tsubprocess.check_call(\n",
				"\t\t\t[\"7z\", \"a\", \"-mx=9\", \"-t7z\", archive_file] +\n",
				"\t\t\t[f for f in files if os.path.isfile(f)]\n",
				"\t\t)\n",
				"\n",
				"\telse:\n",
				"\t\twith zipfile.ZipFile(archive_file, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
				"\t\t\tfor f in files:\n",
				"\t\t\t\tif os.path.isfile(f):\n",
				"\t\t\t\t\tarcname = os.path.relpath(f, model_root)\n",
				"\t\t\t\t\tzf.write(f, arcname)\n",
				"\n",
				"# Export logic\n",
				"debug(f\"Export mode: {Mode}\")\n",
				"if Mode.lower().startswith(\"train\"):\n",
				"\tpack_directory(model_dir, archive_path)\n",
				"else:\n",
				"\tweight_files = [\n",
				"\t\tos.path.join(model_dir, f)\n",
				"\t\tfor f in os.listdir(model_dir)\n",
				"\t\tif f.startswith(f\"{Model_Name}_\")\n",
				"\t\tand f.endswith(\".pth\")\n",
				"\t]\n",
				"\n",
				"\tif not weight_files:\n",
				"\t\traise FileNotFoundError(\"Model has no weight file, please finish training first\")\n",
				"\n",
				"\tweight_files.sort(key = os.path.getmtime, reverse = True)\n",
				"\tweight_file = weight_files[0]\n",
				"\tindex_file = os.path.join(model_dir, f\"{Model_Name}.index\")\n",
				"\n",
				"\tdebug(f\"Selected weight: {weight_file}\")\n",
				"\tpack_files((weight_file, index_file), archive_path)\n",
				"\n",
				"# Move to Drive or download\n",
				"if os.path.ismount(Path_Drive_Parent):\n",
				"\tos.makedirs(Path_Export, exist_ok = True)\n",
				"\tfinal_path = os.path.join(Path_Export, os.path.basename(archive_path))\n",
				"\tshutil.move(archive_path, final_path)\n",
				"\tprint(f\"Exported model to {final_path}\")\n",
				"else:\n",
				"\tprint(f\"Drive not mounted, downloading {os.path.basename(archive_path)}\")\n",
				"\tfiles.download(archive_path)"
			]
		}
	]
}